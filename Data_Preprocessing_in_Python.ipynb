{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM71M4FMy9OPT7z2cqFRYcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritish-tripathy-aiml/Python-AI-Machine-Learning/blob/main/Data_Preprocessing_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing in Python**"
      ],
      "metadata": {
        "id": "Mh5vs9WhCJFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hoB5ykQQ9le3",
        "outputId": "35df4270-f0c2-4b4c-a410-6a621ce1ff85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEvaluation\\n-> Calculate Performance Metrics\\n-> Make a verdict\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# The Machine Learning Process\n",
        "# 1\n",
        "'''\n",
        "Data Preprocessing\n",
        "-> Import the data\n",
        "-> Clean the Data\n",
        "-> Split into training and test Sets\n",
        "'''\n",
        "# 2\n",
        "'''\n",
        "Modelling\n",
        "-> Build the Model\n",
        "-> Train the Model\n",
        "-> Make Predictions\n",
        "'''\n",
        "# 3\n",
        "'''\n",
        "Evaluation\n",
        "-> Calculate Performance Metrics\n",
        "-> Make a verdict\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Set and Test Set\n",
        "'''\n",
        "We always split data into two parts that is train set and test set before performing any Function\n",
        "Train = 80% and Test = 20%\n",
        "Train Set => ŷ = b₀ + b₁X₁ + b₂X₂ then applying the model on test set to know the actual values\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hDZMX8dPCRhm",
        "outputId": "3e2cdde6-a3d9-423e-a3eb-50f0c553ba29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe always split data into two parts that is train set and test set before performing any Function\\nTrain = 80% and Test = 20%\\nTrain Set => ŷ = b₀ + b₁X₁ + b₂X₂ then applying the model on test set to know the actual values\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "'''\n",
        "Normalization => X' = (X - Xmin) / Xmax - Xmin\n",
        "Standardization => X' = X - mu/sigma\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O_CtuGJcDphZ",
        "outputId": "ca8bba42-75a8-4d10-8c53-c2fd102d1da7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nNormalization => X' = (X - Xmin) / Xmax - Xmin\\nStandardization => X' = X - mu/sigma\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Used -> Part 1 - Data Preprocessing/Section 2/Python/Data.csv"
      ],
      "metadata": {
        "id": "vJoHyaO3GFch"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing the Libraries**"
      ],
      "metadata": {
        "id": "lFTX_8N1G9_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np    # To work with Arrays\n",
        "import matplotlib.pyplot as plt   # To make charts and diagrams\n",
        "import pandas as pd     # To import dataset and to create matrix for variables"
      ],
      "metadata": {
        "id": "LXkxgiRVHXu0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing the Dataset**"
      ],
      "metadata": {
        "id": "QoUCVrMIJejl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/Data.csv')\n",
        "'''\n",
        "Features/Independent Variables will be before the Dependent Variables Columns and Dependent Variables in the Last Column.\n",
        "Dependent Variables we need to Predict\n",
        "'''\n",
        "# Takes all the Columns except the Last One\n",
        "X = dataset.iloc[:, :-1].values      # Stores Features in X\n",
        "\n",
        "# Takes the Dependent Variable Columns\n",
        "y = dataset.iloc[:, -1].values      # Stores Dependent Variables in y"
      ],
      "metadata": {
        "id": "yrvtXjRYJiIb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWp29-qgMk0u",
        "outputId": "b8c511b0-e99e-4438-f582-27f4c6b95ddc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dy97-zaMl2J",
        "outputId": "f7adccf7-c594-4b23-a192-8c4bd55aea15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Taking Care of Missing Data**"
      ],
      "metadata": {
        "id": "JPd7MeA3Mr1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "If their is huge amount of data and missing details is 1% or less than 1% then:\n",
        "You can just delete the missing data and create the model, It will not affect the model much\n",
        "But if you have a smaller dataset you need to take care of the empty values\n",
        "'''"
      ],
      "metadata": {
        "id": "Hy3PDv2IOxlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3d98b88f-fbec-419b-cfce-fe94e27847ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIf their is huge amount of data and missing details is 1% or less than 1% then:\\nYou can just delete the missing data and create the model, It will not affect the model much\\nBut if you have a smaller dataset you need to take care of the empty values\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing missing salary with average of all Salary\n",
        "from sklearn.impute import SimpleImputer    # Used to take care of Empty Values\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "SuEna6g6NHyx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRvekbo4QdNl",
        "outputId": "912522cb-0fa6-4d6a-bdcb-3fd92c713d9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Encoding Categorical Data**"
      ],
      "metadata": {
        "id": "OEKpQNTjQfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Encoding means making Categories of the Countries using Binary Bits\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yp1CRThlQzF4",
        "outputId": "b9f00f11-8a1c-4d75-9c27-c1d907ca9a5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEncoding means making Categories of the Countries using Binary Bits\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Encoding Independent Variables/Features Data**"
      ],
      "metadata": {
        "id": "nmB7mbZDRuu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "wXnciF5VR0QO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhGLwtsZTerI",
        "outputId": "96782b46-50f4-46f4-b357-61cdeefb71ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Encoding Dependent Variables/Features Data**"
      ],
      "metadata": {
        "id": "pKd-sQwaTfyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "bdvQaGfGFKM2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otrn1xWtFlI_",
        "outputId": "08ec0f3a-4666-45d2-a148-123035930105"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Splitting the Dataset into Training and Test Set**"
      ],
      "metadata": {
        "id": "4Y_fhqR7FmQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We have to do Feature Scaling after we Split the Dataset into Training and Test Set\n",
        "to prevent Resource Leakage as Test Set is a complete new Dataset on which we evaluate our model with Observations\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SpL4OkxHF-ls",
        "outputId": "e5ace0c0-38e0-4724-bb4f-ab5cacef6023"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe have to do Feature Scaling after we Split the Dataset into Training and Test Set\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the Dataset into Training and Test Set\n",
        "'''\n",
        "Training Set -> We apply our model in Training Set\n",
        "Test Set -> It is like a completely new set with which we test our Model\n",
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "YUOhfKnkGbcQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU6N2MRLILqe",
        "outputId": "65b89ed6-dd40-415d-bd91-c3eb4cce0ca5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiF0cBp3ISWA",
        "outputId": "760856a4-f8d9-4ab4-bf67-3a45890a0df6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2DjYScYITet",
        "outputId": "2b9a1be5-991b-4fce-b74a-d73e2f8051c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPDNejzGIU7T",
        "outputId": "26dcd2dd-200e-483d-9bba-08254314d57b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Feature Scaling**"
      ],
      "metadata": {
        "id": "9CyymiGlIWOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In some of the Models some Features dominate other features and the dominated Features are not even Considered in the\n",
        "Machine Learning Model.\n",
        "So, Feature Scaling ensures that all the Features are on the same Scale.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HyOil3kbVQ-a",
        "outputId": "465bcac5-144e-434b-8cd4-b1830b81ab65"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn some of the Models some Features dominate other features and the dominated Features are not even Considered\\nSo, Feature Scaling ensures that all the Features are on the same Scale.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "There are two ways to do Feature Scaling\n",
        "i) Standardisation = (x - mean(x))/standard deviation(x)\n",
        "ii) Normalisation = (x - min(x))/(max(x) - min(x))\n",
        "x = means all the values\n",
        "Standardisation is recommended for feature scaling because Normalisation works well with values which have Normal Distribution\n",
        "whereas Standardisation does all kind of Distribution perfectly\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2L3X5WX1VtoA",
        "outputId": "01d85dd5-fd94-4c0c-f29c-fed418585c53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThere are two ways to do Feature Scaling\\ni) Standardisation = (x - mean(x))/standard deviation(x)\\nii) Normalisation = (x - min(x))/(max(x) - min(x))\\nx = means all the values\\nStandardisation is recommended for feature scaling because Normalisation works well with values which have Normal Distribution\\nwhereas Standardisation does all kind of Distribution perfectly\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling should be applied to Numerical Values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "metadata": {
        "id": "8gXJ7U0pYA9g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbqlqnNwaupe",
        "outputId": "961add6a-55e6-42f2-b5d2-c11b83a079a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3I1e--Aay0-",
        "outputId": "447431e9-23b9-48f7-a0c7-8b5c46020ca0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**End of Data Preprocessing in Python**"
      ],
      "metadata": {
        "id": "UrHHKuB9a0Nu"
      }
    }
  ]
}